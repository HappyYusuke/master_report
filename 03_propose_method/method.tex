\section{概要}
本研究では、全方位3D LiDARを用いた人追従走行において、オクルージョン発生後のロスト
状態から自律的に復帰するため、LiDARベースの人物再同定モデル「ReID3D」を組み込んだ
システムを提案する。本システムは、主に「歩行者検出」「追従対象の登録・追跡」「オクル
ージョン後の再識別」の3つのフェーズから構成される。

まず、環境内の歩行者位置を特定するための検出モデルとして、3次元点群物体検出ネット
ワークである「PointPillars」を採用する。公開されているPointPillarsの学習済みモデル
は、回転式LiDAR（Velodyne HDL-64E）で収集されたKITTIデータセット
\cite{Are we ready for autonomous driving? the KITTI vision benchmark suite}
を用いているが、本研究で使用するプリズムスキャン方式のLivox Mid-360とはスキャンパ
ターンや点群密度が大きく異なる。このセンサ方式の違いに起因するドメインギャップは
検出精度の低下を招くため、本研究ではLivox Mid-360を用いて独自に構築した歩行者データ
セットを作成し、PointPillarsの再学習（ファインチューニング）を行うことで、
本システムに適した検出器を構築する。

システムの具体的な処理フローは次の通りである。まず、再学習されたPointPillarsを用い
て周囲の歩行者を検出する。追従開始時には、指定された対象の点群からReID3Dを用いて
特徴量を抽出し、追従対象として登録する。通常時は、検出位置に基づくトラッキングに
より対象への追従走行を行う。追従中に障害物による遮蔽（オクルージョン）が発生し
トラッキングが途切れた場合は、再識別フェーズへと移行する。このフェーズでは、
視野内で検出された全ての歩行者の点群をReID3Dに入力し、事前に登録された対象の特徴量
と比較照合を行う。これにより、オクルージョン後の環境から追従対象を正しく再識別し、
追従動作の自律的な再開を実現する。



\section{システム構成}
本研究で提案する人追従システムの全体構成を Fig. \ref{System overview.} に示す。
本システムは Robot Operating System 2 (ROS 2) をミドルウェアとして構築されており、
歩行者検出モデルには、PointPillars の ROS 2 実装である ros2\_tao\_pointpillars 
を採用した。 Fig. \ref{System overview.} は主に追従実行時における処理フローを示し
ているが、本システムの動作は、追従開始直後の「追従対象者の登録フェーズ」と、その後
の「追従フェーズ」で処理内容が大きく異なる。したがって、以下ではこれら2つのフェーズ
に分けて詳細を述べる。

\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=100mm,clip]{figure/System_overview.png}
    \caption{System overview.}
    \label{System overview.}
    \end{center}
\end{figure*}

\paragraph{登録フェーズ}
本フェーズでは、まず3D LiDARから取得された点群データを ros2\_tao\_pointpillars へ
入力し、周囲の歩行者検出を行う。一般に、追従対象の初期化においてはロボットに最も近
接した検出結果を選定する手法がとられるが、検出器の誤認識（False Positive）により、
近傍の壁や障害物が誤って人物として検出される可能性がある。 このような非人物物体の
誤登録を防止するため、本システムでは探索範囲に空間的な制約を設ける。具体的には、
ロボット座標系において横幅 1.0 m、奥行き 3.0 m の矩形領域を設定し、この領域内に
存在する検出結果の中で、かつロボットに最も近い人物を追従対象として選定する。
対象が決定された後、システムは当該人物のトラッキングを開始すると同時に、ReID3Dを
用いて対象の点群から特徴量を抽出し、追従対象として登録する。

\paragraph{追従フェーズ}
本フェーズでは、3D LiDARから取得した点群データを ros2\_tao\_pointpillars へ入力し、
常時歩行者の検出を行う。検出された歩行者はトラッキングアルゴリズムによって追跡され、
前フレームまでの情報に基づいて追従対象との対応付けが行われる。システムはこのトラッ
キング結果に基づき、ロボットの移動制御指令を生成し、対象への追従走行を実行する。
一方、オクルージョン等により追従対象のトラッキングが 1.0 秒間途絶えた場合、システ
ムは対象を見失った（ロスト状態）と判定し、再識別処理へと移行する。 再識別処理では、
その時点で視野内に検出されている全ての歩行者の点群を ReID3D に入力して特徴量を抽出
する。得られた各歩行者の特徴量と、登録フェーズで事前に保存された追従対象の特徴量と
を比較照合することで、追従対象の再同定を行う。

\section{ソフトウェア構成}
ロボット用ノートPC内のソフトウェア構成をFig. \ref{Software stack.}に示す。
ロボット用ノートPCは、RTX 3070 8GB を搭載しており、オペレーティングシステムには
Ubuntu 22.04 LTSを使用した。深層学習モデルであるPointPillarsおよびReID3Dの実装に
あたっては，それぞれ独立したDockerコンテナ環境を構築している．これは，両モデルが
要求するCUDAツールキットやドライバのバージョンが異なり，単一のホスト環境内での共存
が困難であるという依存関係の問題を解決するためである． ミドルウェアにはROS2 Foxy
Fitzroyを採用しており，各DockerコンテナおよびホストPCはROS2の通信プロトコルを介
して連携している．また，PointPillarsによる検出からReID3Dによる識別，そしてロボット
の制御に至る一連のパイプラインを統合管理するため，独自のROS 2パッケージ「HARRP
 (Human-following Autonomous Robot system with ReID3D and PointPillars)」を開発
 した．本システムは，このHARRPパッケージを起動することで，分散されたコンテナ環境
 を意識することなく，統合システムとして実行可能である．

\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=120mm,clip]{figure/Software_stack.png}
    \caption{Software stack.}
    \label{Software stack.}
    \end{center}
\end{figure*}



\section{データセットの作成}
\subsection{データ収集}
データ収集環境をFig.\ref{Data collection environment.}に示す。実験室の広さは
$6.0\,\mathrm{m} \times 7.1\,\mathrm{m}$ である。LiDARは
Fig.\ref{LiDAR and custom stand.}に示すように専用スタンドに固定し、部屋の中央に設置した。
本実験では、実機ロボットへの搭載を模擬するため、
Fig.\ref{LiDAR and custom stand.}に示すスタンドを独自に製作した。このスタンドは、
Fig.\ref{Height comparison.}に示すロボットのセンサ搭載位置と条件を一致させるよう設計
されており、LiDARの設置高さは地面から $119.5\,\mathrm{mm}$ となるように設定されている。

\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=120mm,clip]{figure/data_collection_room.JPG}
    \caption{Data collection environment.}
    \label{Data collection environment.}
    \end{center}
\end{figure*}

\begin{figure*}[h]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[keepaspectratio, width=7cm, angle=-90]{figure/lidar.JPG}
    \caption{LiDAR and custom stand.}
    \label{LiDAR and custom stand.}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[keepaspectratio, height=7cm]{figure/kachaka_lidar.JPG}
    \caption{Height comparison.}
    \label{Height comparison.}
  \end{minipage}
\end{figure*}

\clearpage

データ収集には5名の被験者が参加した。各被験者は、
Fig.\ref{Overview of subjects captured for data collection (1/2).}および
Fig.\ref{Overview of subjects captured for data collection (2/2).}に示すように、
多様な服装を着用した状態で計測を行った。被験者は
Fig.\ref{Walking patterns in data collection.}に示す5種類の歩行パターンに従って
LiDARの周囲を歩行し、合計で12セットのデータを収集した。ここから各セットにつき85フレーム
をランダムに抽出し、計1020フレームの学習用データを取得した。さらに、検証用データセット
として、異なる環境下において別の被験者3名のデータを収集し、同様に200フレームをランダムに
抽出した。以上を合わせ、合計1220フレームからなるデータセットを構築した。

%===========================================================
% 1ページ目：前半 20枚
%===========================================================
\begin{figure}[p]
    \centering
    % リスト内の改行コードがスペース化しないよう % を付加し、カンマ後のスペースも削除
    \foreach \n [count=\i] in {%
        1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20%
    }{
        \subfigure[]{
            % ↓ ここも念のため確認： \n.JPG の間にスペースがないか
            \includegraphics[width=0.17\linewidth]{figure/\n.png}
        }%
        \ifnum\numexpr\i-((\i-1)/4)*4\relax=0 \par\vspace{1.0mm} \else \hfill \fi
    }
    \caption{Overview of subjects captured for data collection (1/2).}
    \label{Overview of subjects captured for data collection (1/2).}
\end{figure}

%===========================================================
% 2ページ目：後半 13枚
%===========================================================
\begin{figure}[p]
    \centering
    % リスト内の改行コードがスペース化しないよう % を付加し、カンマ後のスペースも削除
    \foreach \n [count=\i] in {%
        21,22,23,24,25,26,27,28,29,30,31,32,33%
    }{
        \subfigure[]{
            % ↓ ここも念のため確認： \n.JPG の間にスペースがないか
            \includegraphics[width=0.2\linewidth]{figure/\n.png}
        }%
        \ifnum\numexpr\i-((\i-1)/4)*4\relax=0 \par\vspace{1mm} \else \hfill \fi
    }
    \caption{Overview of subjects captured for data collection (2/2).}
    \label{Overview of subjects captured for data collection (2/2).}
\end{figure}

\begin{figure}[htbp]
    \centering
    % --- 1段目（3枚） ---
    \subfigure[Straight trajectory.]{
        \includegraphics[width=0.3\linewidth]{figure/traj_straight.png}
    }
    \hfill
    \subfigure[Walking around the sensor.]{
        \includegraphics[width=0.3\linewidth]{figure/traj_curve.png}
    }
    \hfill
    \subfigure[Circular trajectory.]{
        \includegraphics[width=0.3\linewidth]{figure/traj_circle.png}
    }
    
    \vspace{2mm} % 上下の段の間隔調整
    
    % --- 2段目（2枚） ---
    \subfigure[Lateral meandering trajectory.]{
        \includegraphics[width=0.3\linewidth]{figure/traj_side.png}
    }
    \quad % 画像間のスペース（\hfillだと端に寄ってしまうため\quad等で調整）
    \subfigure[Longitudinal meandering trajectory.]{
        \includegraphics[width=0.3\linewidth]{figure/traj_updown.png}
    }
    
    \caption{Walking patterns in data collection.}
    \label{Walking patterns in data collection.}
\end{figure}

\subsection{アノテーション}
アノテーション作業の様子をFig. \ref{Annotation using BAT 3D.}に示す。本研究ではアノテ
ーションに点群データのみを用い、ツールとしてBAT 3Dを採用した。BAT 3Dは、Webブラウザ上で
動作するオープンソースツールである3D BAT
\cite{3D BAT: A Semi-Automatic, Web-based 3D Annotation Toolbox for 
    Full-Surround, Multi-Modal Data Streams} 
の後継ソフトウェアである。収集した点群データに対し、歩行者（Pedestrian）のバウンディング
ボックスの付与を行った。BAT 3Dでは、バウンディングボックスを配置すると、
Fig.\ref{Annotation using BAT 3D.}に示すように画面左側にXYZ各軸方向からの視点（三面図）
が表示され、画面右側には位置や姿勢を調整するインターフェースが表示される。この機能により、
3次元空間上での細かな位置合わせが容易となり、歩行者の正確なアノテーションが可能となった。

\begin{figure*}[h]
    \begin{center}
    \includegraphics[width=150mm,clip]{figure/bat-3d.png}
    \caption{Annotation using BAT 3D.}
    \label{Annotation using BAT 3D.}
    \end{center}
\end{figure*}

\subsection{データ拡張}
データ拡張の適用例をFig. \ref{Examples of data augmentation.}に示す。本研究では、
学習データの多様性を向上させるため、独自に実装したプログラムを用いて点群データの拡張を
行った。拡張には、主に以下の3種類の手法を採用した。

第一に、Ground Truth Sampling (GT-Sampling) である。これは、学習データセットに含まれる
歩行者の点群をランダムに抽出し、対象のフレームに追加配置する手法である。 第二に、ローカル
変換 (Local Transformation) である。各歩行者の点群に対し、個別に回転、スケーリング
（拡大縮小）、位置ノイズおよび点群ノイズの付与を行う。 第三に、グローバル変換 
(Global Transformation) である。フレーム内の点群全体に対して、一括して回転、スケーリング
、左右反転を適用する。

各手法における具体的なパラメータ設定をTable \ref{Data augmentation parameters.}に示す。
これらの処理により、学習用データ1,020フレームから15,000フレーム、検証用データ200フレーム
から4,500フレームを生成し、合計19,500フレームのデータセットを構築した。

\begin{figure}[htbp]
    \centering
    \subfigure[Original point cloud.]{
        \includegraphics[width=0.45\linewidth]{figure/PointCloud_origin.png}
    }
    \hfill
    \subfigure[Augmented point cloud.]{
        \includegraphics[width=0.45\linewidth]{figure/PointCloud_aug.png}
    }
    \hfill
    \caption{Examples of data augmentation.}
    \label{Examples of data augmentation.}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Data augmentation parameters.}
  \label{Data augmentation parameters.}
  \begin{tabular}{lr} \toprule
    Parameter & Value \\ \midrule
    
    % GT-Sampling Settings
    \multicolumn{2}{l}{\textbf{GT-Sampling Settings}} \\ 
    Maximum number of added persons & $20$ \\
    $x$-coordinate range [m] & $-6$ to $10$ \\
    $y$-coordinate range [m] & $-6$ to $6$ \\ \midrule

    % Local Transformation Settings
    \multicolumn{2}{l}{\textbf{Local Transformation Settings}} \\
    Rotation around $z$-axis [rad] & $-\pi/2$ to $\pi/2$ \\
    Rotation around $x$- and $y$-axes [rad] & $-\pi/36$ to $\pi/36$ \\
    Scale  & $0.95$ to $1.05$ \\
    Position noise std. dev. [m] & $0.1$ \\
    Point clouds noise std. dev. [m] & $0.01$ \\ \midrule

    % Global Transformation Settings
    \multicolumn{2}{l}{\textbf{Global Transformation Settings}} \\
    Global rotation [rad] & $-\pi/4$ to $\pi/4$ \\
    Global scale & $0.95$ to $1.05$ \\
    Flip probability & $0.5$ \\ \bottomrule
  \end{tabular}
\end{table}

\section{PointPillarsの学習}
PointPillarsの学習には、前節で構築したデータセットを使用した。学習はNVIDIAのTAO Toolkit
を用いて実施した。TAO Toolkitは、深層学習モデルの効率的な学習と最適化を支援するフレーム
ワークであり、PointPillarsの学習済みモデルも提供されている。本研究では、TAO Toolkitの
PointPillars学習パイプラインを利用し、前節で構築したデータセットを用いてファインチュー
ニングを行った。また、学習にはRTX 3090 24GBを2枚搭載したデスクトップPCで学習を行った。
学習に使用したハイパーパラメータを
Table\ref{Hyper parameters for PointPillars training.}に示す。

\begin{table}[h]
  \centering
  \caption{Hyper parameters for PointPillars training.}
  \label{Hyper parameters for PointPillars training.}
  \begin{tabular}{lr} \toprule
    Parameter & Value \\ \midrule
    batch\_size\_per\_gpu & 8 \\
    num\_epochs & 800 \\
    optimizer & adam\_onecycle \\
    lr & 0.003 \\
    weight\_decay & 0.01 \\
    momentum & 0.9 \\
    moms & [0.95, 0.85] \\
    pct\_start & 0.4 \\
    div\_factor & 10.0 \\
    decay\_step\_list & [35, 45] \\
    lr\_decay & 0.1 \\
    lr\_clip & 0.0000001 \\
    lr\_warmup & False \\
    warmup\_epoch & 1 \\
    grad\_norm\_clip & 10.0 \\ \bottomrule
  \end{tabular}
\end{table}

本モデルの学習におけるハイパーパラメータの設定について述べる。学習は800エポックにわたり
実施し、GPUあたりのバッチサイズは8とした。最適化アルゴリズムにはAdamを採用し、学習率の
スケジューリングにはOne Cycle Policyを適用した。One Cycle Policyは、学習の進行に伴い
学習率を初期値から最大値まで上昇させた後、再び減少させる手法であり、これにより学習の高速
化と過学習の抑制を両立させている。また、初期学習率（Base Learning Rate）は0.003とし、
全学習ステップのうち40\%（pct\_start = 0.4）を学習率の上昇フェーズに割り当て、残りの期間
で学習率を減少させる構成とした。また、学習率の変動と連動してモーメンタムを0.95から0.85の
範囲で逆相関的に推移させることで、パラメータ更新の効率化を図っている。さらに、学習の安定性
を確保するために勾配クリッピング（Gradient Norm Clip）を導入した。これは、誤差逆伝播法に
よって算出された勾配のL2ノルムが閾値（本実験では10.0）を超えた際、その大きさを制限する処理
である。この処理により、勾配爆発などの急激なパラメータ変動を防ぎ、安定した学習を実現している。

\section{トラッキング}
トラッキングには、物体の3次元位置 $(x, y, z)$ とそれぞれの速度 $(v_x, v_y, v_z)$ を
状態変数として管理し、観測ノイズが含まれる点群データから真の位置を推定するために、等速
直線運動モデルに基づいた線形カルマンフィルタを実装した。このアルゴリズムは、物理モデル
に基づいて次時刻の状態を推測する「予測ステップ」と、センサの観測値を用いて推定値を補正
する「更新ステップ」の2段階で構成される。

\subsection{状態変数とモデルの定義}

システムの状態ベクトル $\mathbf{x}_k$ は、位置と速度成分を持つ以下の6次元ベクトルとして
定義される。

\begin{equation}
\mathbf{x}_k = \begin{bmatrix} x & y & z & v_x & v_y & v_z \end{bmatrix}^T_k
\end{equation}

物体は微小時間 $dt$ の間、等速で移動すると仮定する。この等速直線運動モデルに基づき、
状態遷移行列（State Transition Matrix） $\mathbf{F}$ は以下のように定義される。
なお、実装においては $dt$ は直前のフレームからの経過時間として動的に更新される。

\begin{equation}
\mathbf{F} = \begin{bmatrix} 
1 & 0 & 0 & dt & 0 & 0 \\
0 & 1 & 0 & 0 & dt & 0 \\
0 & 0 & 1 & 0 & 0 & dt \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 
\end{bmatrix}
\end{equation}

\subsection{予測ステップ (Predict)}

前時刻 $k-1$ の推定値から、現時刻 $k$ の状態を予測する。

\begin{itemize}
    \item \textbf{状態予測 (State Prediction)}:
    現在の速度推定値に基づいて、次の位置を線形予測する。
    \begin{equation}
    \mathbf{\hat{x}}_{k|k-1} = \mathbf{F} \mathbf{\hat{x}}_{k-1|k-1}
    \end{equation}
    
    \item \textbf{誤差共分散行列の予測 (Covariance Prediction)}:
    推定の不確かさを表す誤差共分散行列 $\mathbf{P}$ を更新する。
    ここで $\mathbf{Q}$ はプロセスノイズ共分散行列であり、等速運動モデルからの乖離
    （加速度変動など）の許容量を表す。本実装では対角成分に $0.1$ を設定している。
    \begin{equation}
    \mathbf{P}_{k|k-1} = \mathbf{F} \mathbf{P}_{k-1|k-1} \mathbf{F}^T + \mathbf{Q}
    \end{equation}
\end{itemize}

\subsection{更新ステップ (Update)}

PointPillarsから得られた観測値 $\mathbf{z}_k$ を用いて、予測値を補正する。観測ベクトル
は位置のみの3次元ベクトル $\mathbf{z}_k = [z_x, z_y, z_z]^T$ である。

\begin{itemize}
    \item \textbf{観測モデル (Observation Model)}:
    6次元の状態ベクトルから観測可能な3次元の位置成分を取り出すための観測行列 
    $\mathbf{H}$ を用いる。
    \begin{equation}
    \mathbf{H} = \begin{bmatrix} 
    1 & 0 & 0 & 0 & 0 & 0 \\ 
    0 & 1 & 0 & 0 & 0 & 0 \\ 
    0 & 0 & 1 & 0 & 0 & 0 
    \end{bmatrix}
    \end{equation}

    \item \textbf{カルマンゲインの計算 (Kalman Gain)}:
    予測値と観測値の信頼度のバランスを決定するカルマンゲイン $\mathbf{K}_k$ を算出する。
    ここで $\mathbf{R}$ は観測ノイズ共分散行列であり、センサの測定誤差を表す。本実装では
    $\mathbf{R} = 1.0 \cdot \mathbf{I}$ と設定している。
    \begin{align}
    \mathbf{S}_k &= \mathbf{H} \mathbf{P}_{k|k-1} \mathbf{H}^T + \mathbf{R} \\
    \mathbf{K}_k &= \mathbf{P}_{k|k-1} \mathbf{H}^T \mathbf{S}_k^{-1}
    \end{align}

    \item \textbf{状態の更新 (State Update)}:
    観測残差 
    $\mathbf{y}_k = \mathbf{z}_k - \mathbf{H}\mathbf{\hat{x}}_{k|k-1}$ 
    にカルマンゲインを乗じて予測値を補正し、最終的な推定値を得る。
    \begin{equation}
    \mathbf{\hat{x}}_{k|k} = \mathbf{\hat{x}}_{k|k-1} + \mathbf{K}_k (\mathbf{z}_k - \mathbf{H}\mathbf{\hat{x}}_{k|k-1})
    \end{equation}

    \item \textbf{誤差共分散行列の更新 (Covariance Update)}:
    観測情報の反映に伴い、推定の不確かさを減少させる。
    \begin{equation}
    \mathbf{P}_{k|k} = (\mathbf{I} - \mathbf{K}_k \mathbf{H}) \mathbf{P}_{k|k-1}
    \end{equation}
\end{itemize}

\subsection{実装上の特徴}
本実装では、ROS2環境下での非同期なトピック受信に対応するため、固定のタイムステップでは
なく、前回の更新時刻からの経過時間 $dt$ を用いて状態遷移行列 $\mathbf{F}$ を動的に計算
している。また、初期化時には誤差共分散行列 $\mathbf{P}$ を $10.0 \cdot \mathbf{I}$ と
大きく設定することで、追跡開始直後は観測値を重視し、急速に真値へ収束するよう設計されている。