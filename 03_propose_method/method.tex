\section{概要}
本研究では、全方位3D LiDARを用いた人追従走行において、オクルージョン発生後のロスト
状態から自律的に復帰するため、LiDARベースの人物再同定モデル「ReID3D」を組み込んだ
システムを提案する。本システムは、主に「歩行者検出」「追従対象の登録・追跡」「オクル
ージョン後の再識別」の3つのフェーズから構成される。

まず、環境内の歩行者位置を特定するための検出モデルとして、3次元点群物体検出ネット
ワークである「PointPillars」を採用する。公開されているPointPillarsの学習済みモデル
は、回転式LiDAR（Velodyne HDL-64E）で収集されたKITTIデータセット
\cite{Are we ready for autonomous driving? the KITTI vision benchmark suite}
を用いているが、本研究で使用するプリズムスキャン方式のLivox Mid-360とはスキャンパ
ターンや点群密度が大きく異なる。このセンサ方式の違いに起因するドメインギャップは
検出精度の低下を招くため、本研究ではLivox Mid-360を用いて独自に構築した歩行者データ
セットを作成し、PointPillarsの再学習（ファインチューニング）を行うことで、
本システムに適した検出器を構築する。

システムの具体的な処理フローは次の通りである。まず、再学習されたPointPillarsを用い
て周囲の歩行者を検出する。追従開始時には、指定された対象の点群からReID3Dを用いて
特徴量を抽出し、追従対象として登録する。通常時は、検出位置に基づくトラッキングに
より対象への追従走行を行う。追従中に障害物による遮蔽（オクルージョン）が発生し
トラッキングが途切れた場合は、再識別フェーズへと移行する。このフェーズでは、
視野内で検出された全ての歩行者の点群をReID3Dに入力し、事前に登録された対象の特徴量
と比較照合を行う。これにより、オクルージョン後の環境から追従対象を正しく再識別し、
追従動作の自律的な再開を実現する。



\section{システム構成}
本研究で提案する人追従システムの全体構成を Fig. \ref{System overview.} に示す。
本システムは Robot Operating System 2 (ROS 2) をミドルウェアとして構築されており、
歩行者検出モデルには、PointPillars の ROS 2 実装である ros2\_tao\_pointpillars 
を採用した。 Fig. \ref{System overview.} は主に追従実行時における処理フローを示し
ているが、本システムの動作は、追従開始直後の「追従対象者の登録フェーズ」と、その後
の「追従フェーズ」で処理内容が大きく異なる。したがって、以下ではこれら2つのフェーズ
に分けて詳細を述べる。

\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=100mm,clip]{figure/System_overview.png}
    \caption{System overview.}
    \label{System overview.}
    \end{center}
\end{figure*}

\paragraph{登録フェーズ}
本フェーズでは、まず3D LiDARから取得された点群データを ros2\_tao\_pointpillars へ
入力し、周囲の歩行者検出を行う。一般に、追従対象の初期化においてはロボットに最も近
接した検出結果を選定する手法がとられるが、検出器の誤認識（False Positive）により、
近傍の壁や障害物が誤って人物として検出される可能性がある。 このような非人物物体の
誤登録を防止するため、本システムでは探索範囲に空間的な制約を設ける。具体的には、
ロボット座標系において横幅 1.0 m、奥行き 3.0 m の矩形領域を設定し、この領域内に
存在する検出結果の中で、かつロボットに最も近い人物を追従対象として選定する。
対象が決定された後、システムは当該人物のトラッキングを開始すると同時に、ReID3Dを
用いて対象の点群から特徴量を抽出し、追従対象として登録する。

\paragraph{追従フェーズ}
本フェーズでは、3D LiDARから取得した点群データを ros2\_tao\_pointpillars へ入力し、
常時歩行者の検出を行う。検出された歩行者はトラッキングアルゴリズムによって追跡され、
前フレームまでの情報に基づいて追従対象との対応付けが行われる。システムはこのトラッ
キング結果に基づき、ロボットの移動制御指令を生成し、対象への追従走行を実行する。
一方、オクルージョン等により追従対象のトラッキングが 1.0 秒間途絶えた場合、システ
ムは対象を見失った（ロスト状態）と判定し、再識別処理へと移行する。 再識別処理では、
その時点で視野内に検出されている全ての歩行者の点群を ReID3D に入力して特徴量を抽出
する。得られた各歩行者の特徴量と、登録フェーズで事前に保存された追従対象の特徴量と
を比較照合することで、追従対象の再同定を行う。

\section{ソフトウェア構成}
ロボット用ノートPC内のソフトウェア構成をFig. \ref{Software stack.}に示す。
ロボット用ノートPCは、RTX 3070 8GB を搭載しており、オペレーティングシステムには
Ubuntu 22.04 LTSを使用した。深層学習モデルであるPointPillarsおよびReID3Dの実装に
あたっては，それぞれ独立したDockerコンテナ環境を構築している．これは，両モデルが
要求するCUDAツールキットやドライバのバージョンが異なり，単一のホスト環境内での共存
が困難であるという依存関係の問題を解決するためである． ミドルウェアにはROS2 Foxy
Fitzroyを採用しており，各DockerコンテナおよびホストPCはROS2の通信プロトコルを介
して連携している．また，PointPillarsによる検出からReID3Dによる識別，そしてロボット
の制御に至る一連のパイプラインを統合管理するため，独自のROS 2パッケージ「HARRP
 (Human-following Autonomous Robot system with ReID3D and PointPillars)」を開発
 した．本システムは，このHARRPパッケージを起動することで，分散されたコンテナ環境
 を意識することなく，統合システムとして実行可能である．

\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=120mm,clip]{figure/Software_stack.png}
    \caption{Software stack.}
    \label{Software stack.}
    \end{center}
\end{figure*}

\section{データセットの作成}
\subsection{データ収集}
データ収集環境をFig.\ref{Data collection environment.}に示す。部屋の広さは
$6.0,\mathrm{m} \times 7.1,\mathrm{m}$である。LiDARは
Fig.\ref{LiDAR and custom stand.}のようにスタンドに固定し、部屋の中央に設置した。
本実験では、LiDARをロボットへ搭載することを想定し、
Fig.\ref{LiDAR and custom stand.}に示す専用スタンドを製作した。このスタンドは、
Fig.\ref{Height comparison.}に示すロボットのセンサ搭載位置と条件を一致させるよう
設計されており、LiDARの設置高さが地面から 119.5 mm となるように設定されている。


\begin{figure*}[h]
    \begin{center}
    \includegraphics[height=120mm,clip]{figure/data_collection_room.JPG}
    \caption{Data collection environment.}
    \label{Data collection environment.}
    \end{center}
\end{figure*}


\begin{figure*}[h]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[keepaspectratio, width=7cm, angle=-90]{figure/lidar.JPG}
    \caption{LiDAR and custom stand.}
    \label{LiDAR and custom stand.}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[keepaspectratio, height=7cm]{figure/kachaka_lidar.JPG}
    \caption{Height comparison.}
    \label{Height comparison.}
  \end{minipage}
\end{figure*}

\clearpage

%===========================================================
% 1ページ目：前半 18枚
%===========================================================
\begin{figure}[p]
    \centering
    % リスト内の改行コードがスペース化しないよう % を付加し、カンマ後のスペースも削除
    \foreach \n [count=\i] in {%
        9496,9497,9498,9499,9500,9502,%
        9503,9504,9505,9506,9507,9508,%
        9509,9510,9511,9512,9513,9514%
    }{
        \subfigure[No.\n]{
            % ↓ ここも念のため確認： \n.JPG の間にスペースがないか
            \includegraphics[width=0.3\linewidth]{figure/IMG_\n.JPG}
        }%
        \ifnum\numexpr\i-((\i-1)/3)*3\relax=0 \par\vspace{1mm} \else \hfill \fi
    }
    \caption{実験画像一覧 (1/2)}
\end{figure}

\subsection{アノテーション}

\subsection{データ拡張}



\section{PointPillarsの再学習}



\section{トラッキング}



\section{ロボット制御}